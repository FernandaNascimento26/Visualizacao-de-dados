{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3237219-60d3-4238-b350-dd0964f6c315",
   "metadata": {},
   "source": [
    "### Per√≠odo 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ed60f2-3be9-4752-b17f-3df98daa7d22",
   "metadata": {},
   "source": [
    "#### Importa√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f54046e-a152-40e6-bfb1-f3f7beb5b007",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nasci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\nasci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words_pt = set(stopwords.words('portuguese'))\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "\n",
    "nltk.download('punkt')      # cl√°ssico\n",
    "nltk.download('punkt_tab')  # novo recurso exigido\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c838d724-298f-4295-91f6-7359cf9855b9",
   "metadata": {},
   "source": [
    "### An√°lises iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6207f749-612b-4f39-abd5-70bca0408337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do dataset original: (631150, 33)\n",
      "Linhas com texto (coluna 'text_content_anonymous' n√£o-nula): 260153\n",
      "\n",
      "========== ESTAT√çSTICAS DE TEXTO ==========\n",
      "Total de mensagens com texto: 260153\n",
      "\n",
      "--- num_tokens ---\n",
      "M√©dia: 65.87636890598993\n",
      "Mediana: 18.0\n",
      "Desvio Padr√£o: 394.75748280128863\n",
      "M√≠nimo: 1\n",
      "M√°ximo: 8270\n",
      "\n",
      "--- num_chars ---\n",
      "M√©dia: 352.9082808962418\n",
      "Mediana: 129.0\n",
      "Desvio Padr√£o: 1707.4162382096229\n",
      "M√≠nimo: 1\n",
      "M√°ximo: 65537\n",
      "\n",
      "--- avg_word_length ---\n",
      "M√©dia: 7.801094484093022\n",
      "Mediana: 5.5\n",
      "Desvio Padr√£o: 7.321759754152609\n",
      "M√≠nimo: 1.0\n",
      "M√°ximo: 784.0\n",
      "\n",
      "Exemplo de linhas com as colunas de an√°lise textual:\n",
      "                              text_content_anonymous  \\\n",
      "0  üëÜ *O PIB DECOLA NOVAMENTE!*\\n\\n*O Minist√©rio d...   \n",
      "1  üëÜ *O PIB DECOLA NOVAMENTE!*\\n\\n*O Minist√©rio d...   \n",
      "5                       https://youtu.be/4Kr2KRp6pMk   \n",
      "7  https://m.facebook.com/story.php?story_fbid=pf...   \n",
      "8  *Esta not√≠cia me deixa triste!*\\nhttps://kwai-...   \n",
      "\n",
      "                                              tokens  num_tokens  num_chars  \\\n",
      "0  [üëÜ, *, O, PIB, DECOLA, NOVAMENTE, !, *, *, O, ...          47        206   \n",
      "1  [üëÜ, *, O, PIB, DECOLA, NOVAMENTE, !, *, *, O, ...          47        206   \n",
      "5                 [https, :, //youtu.be/4Kr2KRp6pMk]           3         28   \n",
      "7  [https, :, //m.facebook.com/story.php, ?, stor...           9        148   \n",
      "8  [*, Esta, not√≠cia, me, deixa, triste, !, *, ht...          11         65   \n",
      "\n",
      "   avg_word_length  \n",
      "0         3.659574  \n",
      "1         3.659574  \n",
      "5         9.333333  \n",
      "7        16.444444  \n",
      "8         5.454545  \n"
     ]
    }
   ],
   "source": [
    "# Carregar dataset original\n",
    "df = pd.read_csv(\"dataset_zap_1.csv\", low_memory=False)\n",
    "\n",
    "print(f\"Tamanho do dataset original: {df.shape}\")\n",
    "\n",
    "# =============================================================================\n",
    "# FILTRAR APENAS LINHAS QUE POSSUEM TEXTO\n",
    "# =============================================================================\n",
    "df_text = df.dropna(subset=['text_content_anonymous']).copy()\n",
    "\n",
    "print(f\"Linhas com texto (coluna 'text_content_anonymous' n√£o-nula): {df_text.shape[0]}\")\n",
    "\n",
    "# =============================================================================\n",
    "# CRIAR FUN√á√ïES AUXILIARES PARA TOKENIZAR E CALCULAR ESTAT√çSTICAS\n",
    "# =============================================================================\n",
    "def tokenize_text(text):\n",
    "    \"\"\"Tokeniza usando NLTK e retorna lista de tokens.\"\"\"\n",
    "    return nltk.word_tokenize(str(text))\n",
    "\n",
    "def avg_word_length(token_list):\n",
    "    \"\"\"Retorna comprimento m√©dio das palavras em uma lista de tokens.\"\"\"\n",
    "    if not token_list:\n",
    "        return 0\n",
    "    return np.mean([len(t) for t in token_list])\n",
    "\n",
    "# =============================================================================\n",
    "# CRIAR COLUNAS AUXILIARES: TOKENS, N¬∫ DE TOKENS, N¬∫ DE CARACTERES, ETC.\n",
    "# =============================================================================\n",
    "# Cria uma coluna 'tokens' para cada linha com texto\n",
    "df_text['tokens'] = df_text['text_content_anonymous'].apply(tokenize_text)\n",
    "\n",
    "# N√∫mero de tokens\n",
    "df_text['num_tokens'] = df_text['tokens'].apply(len)\n",
    "\n",
    "# N√∫mero de caracteres no texto (excluindo nulos)\n",
    "df_text['num_chars'] = df_text['text_content_anonymous'].apply(lambda x: len(str(x)))\n",
    "\n",
    "# Comprimento m√©dio das palavras\n",
    "df_text['avg_word_length'] = df_text['tokens'].apply(avg_word_length)\n",
    "\n",
    "# =============================================================================\n",
    "# ESTAT√çSTICAS DESCRITIVAS SOBRE AS NOVAS COLUNAS\n",
    "# =============================================================================\n",
    "print(\"\\n========== ESTAT√çSTICAS DE TEXTO ==========\")\n",
    "\n",
    "# 6.1. Quantidade total de mensagens com texto\n",
    "total_messages = len(df_text)\n",
    "print(f\"Total de mensagens com texto: {total_messages}\")\n",
    "\n",
    "# 6.2. N√∫mero de tokens\n",
    "print(\"\\n--- num_tokens ---\")\n",
    "print(\"M√©dia:\", df_text['num_tokens'].mean())\n",
    "print(\"Mediana:\", df_text['num_tokens'].median())\n",
    "print(\"Desvio Padr√£o:\", df_text['num_tokens'].std())\n",
    "print(\"M√≠nimo:\", df_text['num_tokens'].min())\n",
    "print(\"M√°ximo:\", df_text['num_tokens'].max())\n",
    "\n",
    "# 6.3. N√∫mero de caracteres\n",
    "print(\"\\n--- num_chars ---\")\n",
    "print(\"M√©dia:\", df_text['num_chars'].mean())\n",
    "print(\"Mediana:\", df_text['num_chars'].median())\n",
    "print(\"Desvio Padr√£o:\", df_text['num_chars'].std())\n",
    "print(\"M√≠nimo:\", df_text['num_chars'].min())\n",
    "print(\"M√°ximo:\", df_text['num_chars'].max())\n",
    "\n",
    "# 6.4. Comprimento m√©dio das palavras\n",
    "print(\"\\n--- avg_word_length ---\")\n",
    "print(\"M√©dia:\", df_text['avg_word_length'].mean())\n",
    "print(\"Mediana:\", df_text['avg_word_length'].median())\n",
    "print(\"Desvio Padr√£o:\", df_text['avg_word_length'].std())\n",
    "print(\"M√≠nimo:\", df_text['avg_word_length'].min())\n",
    "print(\"M√°ximo:\", df_text['avg_word_length'].max())\n",
    "\n",
    "# =============================================================================\n",
    "# VISUALIZAR ALGUMAS LINHAS COM AS NOVAS COLUNAS\n",
    "# =============================================================================\n",
    "print(\"\\nExemplo de linhas com as colunas de an√°lise textual:\")\n",
    "print(df_text[['text_content_anonymous', 'tokens', 'num_tokens',\n",
    "               'num_chars', 'avg_word_length']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7422207a-1b15-425c-be5b-591d3af60671",
   "metadata": {},
   "source": [
    "### Normaliza√ß√£o e limpeza do texto\n",
    "\n",
    "O WhatsApp tem v√°rios tipos de ru√≠do:\n",
    "\n",
    "emojis\n",
    "\n",
    "figurinhas convertidas em tokens\n",
    "\n",
    "URLs\n",
    "\n",
    "risadas (‚Äúkkkk‚Äù, ‚Äúrsrsrs‚Äù)\n",
    "\n",
    "stopwords sociais (‚Äúbom dia‚Äù, ‚Äúblz‚Äù, ‚Äúoq‚Äù, ‚Äút√°‚Äù, etc.)\n",
    "\n",
    "pontua√ß√£o repetida\n",
    "\n",
    "mensagens de sistema (‚Äúmensagem apagada‚Äù)\n",
    "\n",
    "n√∫meros irrelevantes\n",
    "\n",
    "tokens como ‚Äúimagem‚Äù, ‚Äú√°udio‚Äù, ‚Äúv√≠deo‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89de6ecd-47dc-4b31-ab8e-0eadb7e0e479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords complementares espec√≠ficas para WhatsApp\n",
    "custom_stopwords = {\n",
    "    # cumprimentos / f√≥rmulas sociais\n",
    "    \"bom\", \"dia\", \"bom dia\",\n",
    "    \"boa\", \"tarde\", \"boa tarde\",\n",
    "    \"noite\", \"boa noite\",\n",
    "    \"ol√°\", \"oi\", \"ola\", \"tchau\",\n",
    "\n",
    "    # risadas / interjei√ß√µes\n",
    "    \"kk\", \"kkk\", \"kkkk\", \"kkkkk\",\n",
    "    \"rs\", \"rsrs\", \"rsrsrs\",\n",
    "    \"aff\", \"eita\", \"oxi\", \"oxe\",\n",
    "\n",
    "    # WhatsApp / m√≠dia\n",
    "    \"imagem\", \"figura\", \"sticker\",\n",
    "    \"√°udio\", \"audio\", \"v√≠deo\", \"video\",\n",
    "}\n",
    "\n",
    "# Uni√£o das stopwords padr√£o + complementares\n",
    "stop_words_total = stop_words_pt | custom_stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b59d4fbd-30f6-4247-a93b-99d1ca670373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    1) Converte para string e min√∫sculas\n",
    "    2) Mant√©m URLs e emojis\n",
    "    3) Normaliza emojis para nomes (:smiling_face:) para virarem tokens\n",
    "    4) Tokeniza com NLTK\n",
    "    5) Remove stopwords (padr√£o + custom) e pontua√ß√£o\n",
    "    \"\"\"\n",
    "    # 1) string + lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # 2) normalizar emojis para texto (sem remover)\n",
    "    text = emoji.demojize(text)  # üôÇ -> :slightly_smiling_face:\n",
    "    \n",
    "    # 3) tokenizar (mant√©m URLs inteiras como tokens)\n",
    "    tokens = nltk.word_tokenize(text, language=\"portuguese\")\n",
    "    \n",
    "    tokens_clean = []\n",
    "    for token in tokens:\n",
    "        # remover pontua√ß√£o pura (., !, ?, etc.)\n",
    "        if token in punctuation:\n",
    "            continue\n",
    "        \n",
    "        # remover stopwords (padr√£o + suas extras)\n",
    "        if token in stop_words_total:\n",
    "            continue\n",
    "        \n",
    "        # aqui N√ÉO removemos n√∫meros, N√ÉO removemos URLs, N√ÉO removemos emojis demojizados\n",
    "        tokens_clean.append(token)\n",
    "    \n",
    "    return tokens_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29f92a21-b453-4bae-8f5d-bbcd2f0d7863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              text_content_anonymous  \\\n",
      "0  üëÜ *O PIB DECOLA NOVAMENTE!*\\n\\n*O Minist√©rio d...   \n",
      "1  üëÜ *O PIB DECOLA NOVAMENTE!*\\n\\n*O Minist√©rio d...   \n",
      "5                       https://youtu.be/4Kr2KRp6pMk   \n",
      "7  https://m.facebook.com/story.php?story_fbid=pf...   \n",
      "8  *Esta not√≠cia me deixa triste!*\\nhttps://kwai-...   \n",
      "\n",
      "                                 tokens_preprocessed  \\\n",
      "0  [backhand_index_pointing_up, pib, decola, nova...   \n",
      "1  [backhand_index_pointing_up, pib, decola, nova...   \n",
      "5                    [https, //youtu.be/4kr2krp6pmk]   \n",
      "7  [https, //m.facebook.com/story.php, story_fbid...   \n",
      "8  [not√≠cia, deixa, triste, https, //kwai-video.c...   \n",
      "\n",
      "                                          clean_text  \n",
      "0  backhand_index_pointing_up pib decola novament...  \n",
      "1  backhand_index_pointing_up pib decola novament...  \n",
      "5                       https //youtu.be/4kr2krp6pmk  \n",
      "7  https //m.facebook.com/story.php story_fbid=pf...  \n",
      "8  not√≠cia deixa triste https //kwai-video.com/p/...  \n"
     ]
    }
   ],
   "source": [
    "df = df.dropna(subset=['text_content_anonymous']).copy()\n",
    "\n",
    "df['tokens_preprocessed'] = df['text_content_anonymous'].apply(preprocess_text)\n",
    "df['clean_text'] = df['tokens_preprocessed'].apply(lambda x: \" \".join(x))\n",
    "\n",
    "print(df[['text_content_anonymous', 'tokens_preprocessed', 'clean_text']].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1f9488a-eba9-4604-b83b-2055153cea5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================\n",
      "TOP 20 TOKENS (RAW / BRUTO)\n",
      "==========================\n",
      "*                         3342776\n",
      "9999999                   548161\n",
      ",                         398377\n",
      "‡πí‡πí‡πí‡πí‡πí‡πí‡πí‡πí                  396214\n",
      "de                        337842\n",
      ":                         313137\n",
      ".                         272774\n",
      "e                         260820\n",
      "‚ò†‚ò†‚ò†‚ò†‚ò†‚ò†                    246825\n",
      "o                         243976\n",
      "a                         207702\n",
      "https                     193263\n",
      "que                       192108\n",
      "do                        167855\n",
      "‡πë‡πë‡πë‡πë‡πë‡πë‡πë‡πë                  167318\n",
      "!                         128439\n",
      "‡ß≠‡ß≠‡ß≠‡ß≠‡ß≠‡ß≠‡ß≠‡ß≠                  124400\n",
      "em                        106984\n",
      ")                         102180\n",
      "√©                         99783\n",
      "\n",
      "==========================\n",
      "TOP 20 TOKENS (CLEAN / PROCESSADO)\n",
      "==========================\n",
      ":skull_and_crossbones     1234693\n",
      "9999999                   548161\n",
      "‡πí‡πí‡πí‡πí‡πí‡πí‡πí‡πí                  396214\n",
      "skull_and_crossbones      247607\n",
      "https                     197360\n",
      "‡πë‡πë‡πë‡πë‡πë‡πë‡πë‡πë                  167318\n",
      "‡ß≠‡ß≠‡ß≠‡ß≠‡ß≠‡ß≠‡ß≠‡ß≠                  124400\n",
      "bolsonaro                 64166\n",
      "lula                      45526\n",
      ":Brazil                   44617\n",
      "ÿÄÿÅÿÄÿÅÿÄÿÅÿÄÿÅÿÄÿÅÿÄ               43164\n",
      "brasil                    42569\n",
      "‚Ä¢                         37709\n",
      "98                        34204\n",
      "486                       34059\n",
      "‚Å®+593                     34056\n",
      "2822‚Å©~                    34056\n",
      "Brazil                    29792\n",
      "...                       27087\n",
      "‡∏ú‡∏¥‡∏î‡∏∏‡∏ó‡πâ‡πà‡πÄ‡∏∂‡∏≤‡∏á‡∏∑‡∏ú‡∏¥‡∏î‡∏∏‡∏ó‡πâ‡πà‡πÄ‡∏∂‡∏≤‡∏á‡∏∑  26746\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# 1. TOKENS \"BRUTOS\" (ANTES DO PR√â-PROCESSAMENTO)\n",
    "# -------------------------------------------------------------------\n",
    "def tokenize_raw(text):\n",
    "    # tokeniza√ß√£o simples s√≥ para diagn√≥stico\n",
    "    return nltk.word_tokenize(str(text).lower(), language=\"portuguese\")\n",
    "\n",
    "# se ainda n√£o tiver essa coluna, criamos:\n",
    "if 'tokens_raw' not in df.columns:\n",
    "    df['tokens_raw'] = df['text_content_anonymous'].apply(tokenize_raw)\n",
    "\n",
    "raw_counter = Counter()\n",
    "df['tokens_raw'].apply(raw_counter.update)\n",
    "\n",
    "top20_raw = raw_counter.most_common(20)\n",
    "\n",
    "print(\"\\n==========================\")\n",
    "print(\"TOP 20 TOKENS (RAW / BRUTO)\")\n",
    "print(\"==========================\")\n",
    "for token, freq in top20_raw:\n",
    "    print(f\"{token:25} {freq}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. TOKENS P√ìS-PR√â-PROCESSAMENTO\n",
    "#    (voc√™ j√° criou 'tokens_preprocessed' e 'clean_text')\n",
    "# -------------------------------------------------------------------\n",
    "clean_counter = Counter()\n",
    "df['tokens_preprocessed'].apply(clean_counter.update)\n",
    "\n",
    "top20_clean = clean_counter.most_common(20)\n",
    "\n",
    "print(\"\\n==========================\")\n",
    "print(\"TOP 20 TOKENS (CLEAN / PROCESSADO)\")\n",
    "print(\"==========================\")\n",
    "for token, freq in top20_clean:\n",
    "    print(f\"{token:25} {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1178d6-afaf-4233-b917-f406c2e62002",
   "metadata": {},
   "source": [
    "### verificar similaridade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0588e80-5df7-4f5c-8a02-3ac89fd0ccf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Rodando experimento k=2, janela=30s\n",
      "Pares analisados: 386643\n",
      "Arquivo salvo: similaridade_k2_win30s.csv\n",
      "Gr√°fico CCDF salvo como CCDF_k2_win30.png\n",
      "Gr√°fico PDF salvo como PDF_k2_win30.png\n",
      "\n",
      ">>> Rodando experimento k=2, janela=60s\n",
      "Pares analisados: 459253\n",
      "Arquivo salvo: similaridade_k2_win60s.csv\n",
      "Gr√°fico CCDF salvo como CCDF_k2_win60.png\n",
      "Gr√°fico PDF salvo como PDF_k2_win60.png\n",
      "\n",
      ">>> Rodando experimento k=2, janela=90s\n",
      "Pares analisados: 485227\n",
      "Arquivo salvo: similaridade_k2_win90s.csv\n",
      "Gr√°fico CCDF salvo como CCDF_k2_win90.png\n",
      "Gr√°fico PDF salvo como PDF_k2_win90.png\n",
      "\n",
      ">>> Rodando experimento k=5, janela=30s\n",
      "Pares analisados: 674449\n",
      "Arquivo salvo: similaridade_k5_win30s.csv\n",
      "Gr√°fico CCDF salvo como CCDF_k5_win30.png\n",
      "Gr√°fico PDF salvo como PDF_k5_win30.png\n",
      "\n",
      ">>> Rodando experimento k=5, janela=60s\n",
      "Pares analisados: 951701\n",
      "Arquivo salvo: similaridade_k5_win60s.csv\n",
      "Gr√°fico CCDF salvo como CCDF_k5_win60.png\n",
      "Gr√°fico PDF salvo como PDF_k5_win60.png\n",
      "\n",
      ">>> Rodando experimento k=5, janela=90s\n",
      "Pares analisados: 1080681\n",
      "Arquivo salvo: similaridade_k5_win90s.csv\n",
      "Gr√°fico CCDF salvo como CCDF_k5_win90.png\n",
      "Gr√°fico PDF salvo como PDF_k5_win90.png\n",
      "\n",
      ">>> Rodando experimento k=10, janela=30s\n",
      "Pares analisados: 838200\n",
      "Arquivo salvo: similaridade_k10_win30s.csv\n",
      "Gr√°fico CCDF salvo como CCDF_k10_win30.png\n",
      "Gr√°fico PDF salvo como PDF_k10_win30.png\n",
      "\n",
      ">>> Rodando experimento k=10, janela=60s\n",
      "Pares analisados: 1362620\n",
      "Arquivo salvo: similaridade_k10_win60s.csv\n",
      "Gr√°fico CCDF salvo como CCDF_k10_win60.png\n",
      "Gr√°fico PDF salvo como PDF_k10_win60.png\n",
      "\n",
      ">>> Rodando experimento k=10, janela=90s\n",
      "Pares analisados: 1722954\n",
      "Arquivo salvo: similaridade_k10_win90s.csv\n",
      "Gr√°fico CCDF salvo como CCDF_k10_win90.png\n",
      "Gr√°fico PDF salvo como PDF_k10_win90.png\n",
      "\n",
      "Resumo completo salvo em resumo_experimentos_similaridade.csv\n",
      "    k  window_seconds  num_pairs  mean_similarity  median_similarity  \\\n",
      "0   2              30     386643         0.106053           0.003223   \n",
      "1   2              60     459253         0.095131           0.002231   \n",
      "2   2              90     485227         0.092007           0.001979   \n",
      "3   5              30     674449         0.110306           0.004075   \n",
      "4   5              60     951701         0.087823           0.002047   \n",
      "5   5              90    1080681         0.080973           0.001333   \n",
      "6  10              30     838200         0.133250           0.005682   \n",
      "7  10              60    1362620         0.094545           0.002955   \n",
      "8  10              90    1722954         0.080516           0.001592   \n",
      "\n",
      "   p90_similarity  max_similarity  \n",
      "0        0.260549             1.0  \n",
      "1        0.194027             1.0  \n",
      "2        0.181102             1.0  \n",
      "3        0.301025             1.0  \n",
      "4        0.163277             1.0  \n",
      "5        0.140633             1.0  \n",
      "6        1.000000             1.0  \n",
      "7        0.180235             1.0  \n",
      "8        0.134111             1.0  \n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 4. Rodar todos os experimentos, salvar CSVs e gerar gr√°ficos\n",
    "# ================================\n",
    "ks = [2, 5, 10]\n",
    "windows = [10, 30, 60, 90]\n",
    "limiar_sim = 0.7      # similaridade usada no paper para detectar coordena√ß√£o\n",
    "max_dt_pdf = 10       # intervalo m√°ximo para o gr√°fico da Figura 3\n",
    "\n",
    "summary_rows = []\n",
    "\n",
    "def gerar_ccdf(df_exp, k, win):\n",
    "    \"\"\"Gera gr√°fico CCDF dos intervalos de tempo para similaridade ‚â• 0.7.\"\"\"\n",
    "    df_exp[\"offset\"] = df_exp[\"idx_j\"] - df_exp[\"idx_i\"]\n",
    "\n",
    "    df_high = df_exp[df_exp[\"similarity\"] >= limiar_sim].copy()\n",
    "\n",
    "    if df_high.empty:\n",
    "        print(\"Nenhum par com similaridade alta para CCDF.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(7, 5))\n",
    "\n",
    "    for offset in range(1, k+1):\n",
    "        sub = df_high[df_high[\"offset\"] == offset]\n",
    "\n",
    "        if len(sub) == 0:\n",
    "            continue\n",
    "\n",
    "        ts = np.sort(sub[\"delta_t_seconds\"].values)\n",
    "        n = len(ts)\n",
    "        ccdf = 1 - np.arange(1, n+1) / n\n",
    "\n",
    "        plt.loglog(ts, ccdf, label=f\"+{offset}\")\n",
    "\n",
    "    plt.xlabel(\"Time Interval (seconds)\")\n",
    "    plt.ylabel(\"CCDF\")\n",
    "    plt.title(f\"CCDF ‚Äî Similaridade ‚â• {limiar_sim} (k={k}, janela={win}s)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fname = f\"CCDF_k{k}_win{win}.png\"\n",
    "    plt.savefig(fname, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Gr√°fico CCDF salvo como {fname}\")\n",
    "\n",
    "\n",
    "def gerar_pdf_similaridade(df_exp, k, win):\n",
    "    \"\"\"Gera gr√°fico PDF das similaridades para Œît ‚â§ 10s (paper Figure 3).\"\"\"\n",
    "    df_short = df_exp[df_exp[\"delta_t_seconds\"] <= max_dt_pdf].copy()\n",
    "\n",
    "    if df_short.empty:\n",
    "        print(\"Nenhum par com Œît <= 10s para PDF\")\n",
    "        return\n",
    "\n",
    "    sims = df_short[\"similarity\"].values\n",
    "    bins = np.linspace(0, 1, 20)\n",
    "\n",
    "    hist, edges = np.histogram(sims, bins=bins, density=True)\n",
    "    centers = 0.5 * (edges[:-1] + edges[1:])\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.semilogy(centers, hist, marker=\"o\")\n",
    "    plt.xlabel(\"Text Similarity\")\n",
    "    plt.ylabel(\"PDF\")\n",
    "    plt.title(f\"PDF ‚Äî Pairs with Œît ‚â§ {max_dt_pdf}s (k={k}, janela={win}s)\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fname = f\"PDF_k{k}_win{win}.png\"\n",
    "    plt.savefig(fname, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Gr√°fico PDF salvo como {fname}\")\n",
    "\n",
    "\n",
    "# LOOP DOS EXPERIMENTOS\n",
    "for k in ks:\n",
    "    for win in windows:\n",
    "        print(f\"\\n>>> Rodando experimento k={k}, janela={win}s\")\n",
    "        df_exp = run_experiment(k=k, window_seconds=win)\n",
    "\n",
    "        print(\"Pares analisados:\", len(df_exp))\n",
    "        if len(df_exp) == 0:\n",
    "            continue\n",
    "\n",
    "        # estat√≠sticas b√°sicas\n",
    "        summary_rows.append({\n",
    "            \"k\": k,\n",
    "            \"window_seconds\": win,\n",
    "            \"num_pairs\": len(df_exp),\n",
    "            \"mean_similarity\": df_exp[\"similarity\"].mean(),\n",
    "            \"median_similarity\": df_exp[\"similarity\"].median(),\n",
    "            \"p90_similarity\": df_exp[\"similarity\"].quantile(0.90),\n",
    "            \"max_similarity\": df_exp[\"similarity\"].max(),\n",
    "        })\n",
    "\n",
    "        # salvar CSV dos pares\n",
    "        out_name = f\"similaridade_k{k}_win{win}s.csv\"\n",
    "        df_exp.to_csv(out_name, index=False, encoding=\"utf-8\")\n",
    "        print(f\"Arquivo salvo: {out_name}\")\n",
    "\n",
    "        # ===============================\n",
    "        # GERAR GR√ÅFICOS DO ARTIGO\n",
    "        # ===============================\n",
    "        gerar_ccdf(df_exp, k, win)\n",
    "        gerar_pdf_similaridade(df_exp, k, win)\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 5. Criar CSV resumo\n",
    "# ================================\n",
    "df_summary = pd.DataFrame(summary_rows)\n",
    "df_summary.to_csv(\"resumo_experimentos_similaridade.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"\\nResumo completo salvo em resumo_experimentos_similaridade.csv\")\n",
    "print(df_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b035a49f-6f05-4d54-9161-95d5e6d1eeaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (.venv)",
   "language": "python",
   "name": "py312-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

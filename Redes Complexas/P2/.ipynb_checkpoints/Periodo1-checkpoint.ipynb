{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3237219-60d3-4238-b350-dd0964f6c315",
   "metadata": {},
   "source": [
    "### Per√≠odo 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ed60f2-3be9-4752-b17f-3df98daa7d22",
   "metadata": {},
   "source": [
    "#### Importa√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f54046e-a152-40e6-bfb1-f3f7beb5b007",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nasci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\nasci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words_pt = set(stopwords.words('portuguese'))\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "\n",
    "nltk.download('punkt')      # cl√°ssico\n",
    "nltk.download('punkt_tab')  # novo recurso exigido\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c838d724-298f-4295-91f6-7359cf9855b9",
   "metadata": {},
   "source": [
    "### An√°lises iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6207f749-612b-4f39-abd5-70bca0408337",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset_zap_2.csv\", low_memory=False)\n",
    "\n",
    "df = df.dropna(subset=[\"text_content_anonymous\"]).copy()\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7422207a-1b15-425c-be5b-591d3af60671",
   "metadata": {},
   "source": [
    "### Normaliza√ß√£o e limpeza do texto\n",
    "\n",
    "O WhatsApp tem v√°rios tipos de ru√≠do:\n",
    "\n",
    "emojis\n",
    "\n",
    "figurinhas convertidas em tokens\n",
    "\n",
    "URLs\n",
    "\n",
    "risadas (‚Äúkkkk‚Äù, ‚Äúrsrsrs‚Äù)\n",
    "\n",
    "stopwords sociais (‚Äúbom dia‚Äù, ‚Äúblz‚Äù, ‚Äúoq‚Äù, ‚Äút√°‚Äù, etc.)\n",
    "\n",
    "pontua√ß√£o repetida\n",
    "\n",
    "mensagens de sistema (‚Äúmensagem apagada‚Äù)\n",
    "\n",
    "n√∫meros irrelevantes\n",
    "\n",
    "tokens como ‚Äúimagem‚Äù, ‚Äú√°udio‚Äù, ‚Äúv√≠deo‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89de6ecd-47dc-4b31-ab8e-0eadb7e0e479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords complementares espec√≠ficas para WhatsApp\n",
    "custom_stopwords = {\n",
    "    # cumprimentos / f√≥rmulas sociais\n",
    "    \"bom\", \"dia\",\n",
    "    \"boa\", \"tarde\",\n",
    "    \"noite\", \"boa\", \"noite\",\n",
    "    \"ol√°\", \"oi\", \"ola\", \"tchau\",\n",
    "\n",
    "    # risadas / interjei√ß√µes\n",
    "    \"kk\", \"kkk\", \"kkkk\", \"kkkkk\",\n",
    "    \"rs\", \"rsrs\", \"rsrsrs\",\n",
    "    \"aff\", \"eita\", \"oxi\", \"oxe\",\n",
    "\n",
    "    # WhatsApp / m√≠dia\n",
    "    \"imagem\", \"figura\", \"sticker\",\n",
    "    \"√°udio\", \"audio\", \"v√≠deo\", \"video\",\n",
    "}\n",
    "\n",
    "# Uni√£o das stopwords padr√£o + complementares\n",
    "stop_words_total = stop_words_pt | custom_stopwords\n",
    "\n",
    "punctuation = set(string.punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b59d4fbd-30f6-4247-a93b-99d1ca670373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    1) Lowercase\n",
    "    2) Mant√©m emojis e URLs\n",
    "    3) Normaliza emojis para texto (:smiling_face:)\n",
    "    4) Tokeniza√ß√£o universal do NLTK (sem depender do idioma)\n",
    "    5) Remove stopwords e pontua√ß√£o simples\n",
    "    \"\"\"\n",
    "    # lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # normalizar emojis\n",
    "    text = emoji.demojize(text)\n",
    "    \n",
    "    # tokenizar (universal tokenizer)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    tokens_clean = []\n",
    "    for token in tokens:\n",
    "\n",
    "        # remover pontua√ß√£o simples\n",
    "        if token in punctuation:\n",
    "            continue\n",
    "        \n",
    "        # remover stopwords\n",
    "        if token in stop_words_total:\n",
    "            continue\n",
    "\n",
    "        # manter URLs, emojis, n√∫meros e tokens curtos\n",
    "        tokens_clean.append(token)\n",
    "    \n",
    "    return tokens_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29f92a21-b453-4bae-8f5d-bbcd2f0d7863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mensagens restantes ap√≥s limpeza: (259650, 35)\n",
      "                              text_content_anonymous  \\\n",
      "0  üëÜ *O PIB DECOLA NOVAMENTE!*\\n\\n*O Minist√©rio d...   \n",
      "1  üëÜ *O PIB DECOLA NOVAMENTE!*\\n\\n*O Minist√©rio d...   \n",
      "2                       https://youtu.be/4Kr2KRp6pMk   \n",
      "3  https://m.facebook.com/story.php?story_fbid=pf...   \n",
      "4  *Esta not√≠cia me deixa triste!*\\nhttps://kwai-...   \n",
      "5  Urgente: Pesquisa Brasmarket aponta vit√≥ria de...   \n",
      "6  https://www.facebook.com/marcelo.calderaro.90/...   \n",
      "7  https://youtu.be/9St9wUrpU_c\\n\\nüáßüá∑üáßüá∑üáßüá∑üåøüåøüåøüáßüá∑üáßüá∑üáß...   \n",
      "8  üëÜ *BIOMETRIA NAS URNAS!*\\n\\n*O presidente do T...   \n",
      "9  üëÜ *BIOMETRIA NAS URNAS!*\\n\\n*O presidente do T...   \n",
      "\n",
      "                                 tokens_preprocessed  \\\n",
      "0  [backhand_index_pointing_up, pib, decola, nova...   \n",
      "1  [backhand_index_pointing_up, pib, decola, nova...   \n",
      "2                    [https, //youtu.be/4kr2krp6pmk]   \n",
      "3  [https, //m.facebook.com/story.php, story_fbid...   \n",
      "4  [not√≠cia, deixa, triste, https, //kwai-video.c...   \n",
      "5  [urgente, pesquisa, brasmarket, aponta, vit√≥ri...   \n",
      "6  [https, //www.facebook.com/marcelo.calderaro.9...   \n",
      "7  [https, //youtu.be/9st9wurpu_c, Brazil, :Brazi...   \n",
      "8  [backhand_index_pointing_up, biometria, urnas,...   \n",
      "9  [backhand_index_pointing_up, biometria, urnas,...   \n",
      "\n",
      "                                          clean_text  \n",
      "0  backhand_index_pointing_up pib decola novament...  \n",
      "1  backhand_index_pointing_up pib decola novament...  \n",
      "2                       https //youtu.be/4kr2krp6pmk  \n",
      "3  https //m.facebook.com/story.php story_fbid=pf...  \n",
      "4  not√≠cia deixa triste https //kwai-video.com/p/...  \n",
      "5  urgente pesquisa brasmarket aponta vit√≥ria bol...  \n",
      "6  https //www.facebook.com/marcelo.calderaro.90/...  \n",
      "7  https //youtu.be/9st9wurpu_c Brazil :Brazil :B...  \n",
      "8  backhand_index_pointing_up biometria urnas pre...  \n",
      "9  backhand_index_pointing_up biometria urnas pre...  \n"
     ]
    }
   ],
   "source": [
    "# 1. remover mensagens sem texto original\n",
    "df = df.dropna(subset=['text_content_anonymous']).reset_index(drop=True)\n",
    "\n",
    "# 2. preprocessamento\n",
    "df['tokens_preprocessed'] = df['text_content_anonymous'].apply(preprocess_text)\n",
    "\n",
    "# 3. gerar clean_text\n",
    "df['clean_text'] = df['tokens_preprocessed'].apply(lambda x: \" \".join(x))\n",
    "\n",
    "# 4. remover mensagens cujo texto limpo ficou vazio\n",
    "df = df[df['clean_text'].str.strip() != \"\"].reset_index(drop=True)\n",
    "\n",
    "print(\"Mensagens restantes ap√≥s limpeza:\", df.shape)\n",
    "print(df[['text_content_anonymous', 'tokens_preprocessed', 'clean_text']].head(10))\n",
    "\n",
    "df.to_csv(\"dataset_zap_2_preprocessado.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19f63670-51bb-4399-8ec0-23bc5dcbbaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "times = df[\"date_message\"].tolist()\n",
    "N = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cca06b7-6282-4844-a9dd-2fd3a3d650ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depois de filtrar datas v√°lidas e ordenar: (259650, 35)\n"
     ]
    }
   ],
   "source": [
    "# garantir que date_message √© datetime e n√£o string\n",
    "df = df.dropna(subset=[\"date_message\"]).copy()\n",
    "df[\"date_message\"] = pd.to_datetime(df[\"date_message\"], errors=\"coerce\")\n",
    "\n",
    "# remover linhas com data inv√°lida\n",
    "df = df.dropna(subset=[\"date_message\"]).reset_index(drop=True)\n",
    "\n",
    "# ordenar no tempo (ESSENCIAL para janela temporal)\n",
    "df = df.sort_values(\"date_message\").reset_index(drop=True)\n",
    "\n",
    "print(\"Depois de filtrar datas v√°lidas e ordenar:\", df.shape)\n",
    "\n",
    "# agora sim, criar texts/times/N\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "times = df[\"date_message\"].tolist()\n",
    "N = len(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1178d6-afaf-4233-b917-f406c2e62002",
   "metadata": {},
   "source": [
    "### verificar similaridade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0588e80-5df7-4f5c-8a02-3ac89fd0ccf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ETAPA 2] Gerando representa√ß√£o TF-IDF (com IDF embutido)...\n",
      "Matriz TF-IDF gerada: (259650, 87379)\n",
      "Tabela de IDF salva em 'valores_idf_tfidf.csv'.\n",
      "\n",
      ">>> Rodando experimento k=2, janela=10s\n",
      "Pares analisados: 235779\n",
      "\n",
      ">>> Rodando experimento k=2, janela=30s\n",
      "Pares analisados: 385693\n",
      "\n",
      ">>> Rodando experimento k=2, janela=60s\n",
      "Pares analisados: 458243\n",
      "\n",
      ">>> Rodando experimento k=2, janela=90s\n",
      "Pares analisados: 484209\n",
      "\n",
      ">>> Rodando experimento k=5, janela=10s\n",
      "Pares analisados: 337940\n",
      "\n",
      ">>> Rodando experimento k=5, janela=30s\n",
      "Pares analisados: 672570\n",
      "\n",
      ">>> Rodando experimento k=5, janela=60s\n",
      "Pares analisados: 949392\n",
      "\n",
      ">>> Rodando experimento k=5, janela=90s\n",
      "Pares analisados: 1078210\n",
      "\n",
      ">>> Rodando experimento k=10, janela=10s\n",
      "Pares analisados: 409696\n",
      "\n",
      ">>> Rodando experimento k=10, janela=30s\n",
      "Pares analisados: 835669\n",
      "\n",
      ">>> Rodando experimento k=10, janela=60s\n",
      "Pares analisados: 1358792\n",
      "\n",
      ">>> Rodando experimento k=10, janela=90s\n",
      "Pares analisados: 1718386\n",
      "\n",
      "Resumo completo salvo em resumo_experimentos_similaridade.csv\n",
      "     k  window_seconds  num_pairs  mean_similarity  median_similarity  \\\n",
      "0    2              10     235779         0.147261           0.007008   \n",
      "1    2              30     385693         0.106201           0.003322   \n",
      "2    2              60     458243         0.095233           0.002345   \n",
      "3    2              90     484209         0.092104           0.002102   \n",
      "4    5              10     337940         0.178831           0.011375   \n",
      "5    5              30     672570         0.110684           0.004151   \n",
      "6    5              60     949392         0.088097           0.002172   \n",
      "7    5              90    1078210         0.081219           0.001479   \n",
      "8   10              10     409696         0.221408           0.014790   \n",
      "9   10              30     835669         0.133640           0.005766   \n",
      "10  10              60    1358792         0.094805           0.003053   \n",
      "11  10              90    1718386         0.080737           0.001732   \n",
      "\n",
      "    p90_similarity  max_similarity  \n",
      "0         1.000000             1.0  \n",
      "1         0.262181             1.0  \n",
      "2         0.194524             1.0  \n",
      "3         0.181374             1.0  \n",
      "4         1.000000             1.0  \n",
      "5         0.307043             1.0  \n",
      "6         0.164360             1.0  \n",
      "7         0.141100             1.0  \n",
      "8         1.000000             1.0  \n",
      "9         1.000000             1.0  \n",
      "10        0.181449             1.0  \n",
      "11        0.134673             1.0  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[ETAPA 2] Gerando representa√ß√£o TF-IDF (com IDF embutido)...\")\n",
    "\n",
    "# garantir inputs\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "times = df[\"date_message\"].tolist()\n",
    "N = len(df)\n",
    "\n",
    "# Vetoriza√ß√£o TF-IDF (IDF embutido)\n",
    "vectorizer = TfidfVectorizer(min_df=2)\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "print(\"Matriz TF-IDF gerada:\", X.shape)\n",
    "\n",
    "# salvar valores de IDF para documenta√ß√£o\n",
    "idf_df = pd.DataFrame({\n",
    "    \"term\": vectorizer.get_feature_names_out(),\n",
    "    \"idf\": vectorizer.idf_\n",
    "}).sort_values(by=\"idf\", ascending=False)\n",
    "\n",
    "idf_df.to_csv(\"valores_idf_tfidf.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"Tabela de IDF salva em 'valores_idf_tfidf.csv'.\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 3. FUN√á√ÉO PARA RODAR UM EXPERIMENTO\n",
    "# ============================================\n",
    "\n",
    "def run_experiment(k, window_seconds):\n",
    "    results = []\n",
    "\n",
    "    for i in range(N - k):      # otimiza√ß√£o\n",
    "        vec_i = X[i]\n",
    "        t_i  = times[i]\n",
    "\n",
    "        for d in range(1, k + 1):\n",
    "            j = i + d\n",
    "\n",
    "            t_j = times[j]\n",
    "            delta_t = (t_j - t_i).total_seconds()\n",
    "\n",
    "            if delta_t < 0:\n",
    "                continue\n",
    "            if delta_t > window_seconds:\n",
    "                break\n",
    "\n",
    "            sim_ij = float(vec_i.dot(X[j].T)[0, 0])\n",
    "\n",
    "            results.append({\n",
    "                \"idx_i\": i,\n",
    "                \"idx_j\": j,\n",
    "                \"date_i\": t_i,\n",
    "                \"date_j\": t_j,\n",
    "                \"delta_t_seconds\": delta_t,\n",
    "                \"similarity\": sim_ij,\n",
    "                \"k\": k,\n",
    "                \"window_seconds\": window_seconds,\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 4. EXPERIMENTOS + CSVs + CCDF + PDF\n",
    "# ============================================\n",
    "\n",
    "ks = [2, 5, 10]\n",
    "windows = [10, 30, 60, 90]\n",
    "limiar_sim = 0.7\n",
    "max_dt_pdf = 10\n",
    "\n",
    "summary_rows = []\n",
    "\n",
    "def gerar_ccdf(df_exp, k, win):\n",
    "    df_exp = df_exp.copy()\n",
    "    df_exp[\"offset\"] = df_exp[\"idx_j\"] - df_exp[\"idx_i\"]\n",
    "    df_high = df_exp[df_exp[\"similarity\"] >= limiar_sim]\n",
    "\n",
    "    if df_high.empty:\n",
    "        print(\"Nenhum par com similaridade alta para CCDF.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(7,5))\n",
    "\n",
    "    for offset in range(1, k+1):\n",
    "        sub = df_high[df_high[\"offset\"] == offset]\n",
    "        if len(sub) == 0:\n",
    "            continue\n",
    "\n",
    "        ts = np.sort(sub[\"delta_t_seconds\"])\n",
    "        ccdf = 1 - np.arange(1, len(ts)+1) / len(ts)\n",
    "\n",
    "        plt.loglog(ts, ccdf, label=f\"+{offset}\")\n",
    "\n",
    "    plt.xlabel(\"Time Interval (seconds)\")\n",
    "    plt.ylabel(\"CCDF\")\n",
    "    plt.title(f\"CCDF ‚Äî Similarity ‚â• {limiar_sim} (k={k}, window={win}s)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"CCDF_k{k}_win{win}.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def gerar_pdf_similaridade(df_exp, k, win):\n",
    "    df_short = df_exp[df_exp[\"delta_t_seconds\"] <= max_dt_pdf]\n",
    "    if df_short.empty:\n",
    "        print(\"Nenhum par com Œît <= max_dt_pdf para PDF.\")\n",
    "        return\n",
    "\n",
    "    sims = df_short[\"similarity\"]\n",
    "    hist, edges = np.histogram(sims, bins=np.linspace(0,1,20), density=True)\n",
    "    centers = (edges[:-1] + edges[1:]) / 2\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.semilogy(centers, hist, marker=\"o\")\n",
    "    plt.xlabel(\"Text Similarity\")\n",
    "    plt.ylabel(\"PDF\")\n",
    "    plt.title(f\"PDF ‚Äî Pairs with Œît ‚â§ {max_dt_pdf}s (k={k}, window={win}s)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"PDF_k{k}_win{win}.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# LOOP DOS EXPERIMENTOS\n",
    "for k in ks:\n",
    "    for win in windows:\n",
    "        print(f\"\\n>>> Rodando experimento k={k}, janela={win}s\")\n",
    "        \n",
    "        df_exp = run_experiment(k, win)\n",
    "        print(\"Pares analisados:\", len(df_exp))\n",
    "\n",
    "        if len(df_exp) == 0:\n",
    "            continue\n",
    "\n",
    "        # Estat√≠sticas resumidas\n",
    "        summary_rows.append({\n",
    "            \"k\": k,\n",
    "            \"window_seconds\": win,\n",
    "            \"num_pairs\": len(df_exp),\n",
    "            \"mean_similarity\": df_exp[\"similarity\"].mean(),\n",
    "            \"median_similarity\": df_exp[\"similarity\"].median(),\n",
    "            \"p90_similarity\": df_exp[\"similarity\"].quantile(0.90),\n",
    "            \"max_similarity\": df_exp[\"similarity\"].max(),\n",
    "        })\n",
    "\n",
    "        df_exp.to_csv(f\"similaridade_k{k}_win{win}s.csv\", index=False)\n",
    "        gerar_ccdf(df_exp, k, win)\n",
    "        gerar_pdf_similaridade(df_exp, k, win)\n",
    "\n",
    "\n",
    "# RESUMO FINAL\n",
    "df_summary = pd.DataFrame(summary_rows)\n",
    "df_summary.to_csv(\"resumo_experimentos_similaridade.csv\", index=False)\n",
    "print(\"\\nResumo completo salvo em resumo_experimentos_similaridade.csv\")\n",
    "print(df_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4171111f-25e5-41fa-8f66-33eca12e54dc",
   "metadata": {},
   "source": [
    "### Conjunto coordenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8db5b8d4-616f-4fc5-8fec-12fe864cc63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de pares no experimento: 410773\n",
      "Total de pares coordenados encontrados: 78259\n",
      "Arquivo salvo: conjunto_coordenado_mensagens.csv\n"
     ]
    }
   ],
   "source": [
    "# carregar o experimento ideal\n",
    "df10_10 = pd.read_csv(\"similaridade_k10_win10s.csv\")\n",
    "\n",
    "print(\"Total de pares no experimento:\", len(df10_10))\n",
    "\n",
    "# ================================\n",
    "# FILTRAR PARES COORDENADOS\n",
    "# ================================\n",
    "limiar_sim = 0.7\n",
    "max_dt = 10\n",
    "\n",
    "df_coord = df10_10[\n",
    "    (df10_10[\"similarity\"] >= limiar_sim) &\n",
    "    (df10_10[\"delta_t_seconds\"] <= max_dt)\n",
    "].copy()\n",
    "\n",
    "print(\"Total de pares coordenados encontrados:\", len(df_coord))\n",
    "\n",
    "# salvar CSV do conjunto coordenado\n",
    "df_coord.to_csv(\"conjunto_coordenado_mensagens.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"Arquivo salvo: conjunto_coordenado_mensagens.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dba7db9-224e-43ae-b36a-84da9c79218c",
   "metadata": {},
   "source": [
    "### Rede Mensagem - Mensagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "677a34e8-0e26-4170-8177-da78136d2b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pares coordenados: 78259\n",
      "N√≥s na rede: 36509\n",
      "Arestas na rede: 78259\n",
      "N√∫mero de componentes coordenados: 12182\n",
      "Tamanho do maior cluster coordenado: 874 mensagens\n",
      "Cluster coordenado principal salvo em cluster_coordenado_principal.csv\n",
      "Grafo coordenado salvo como rede_coordenada.gexf (compat√≠vel com Gephi).\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 1. Carregar conjunto coordenado\n",
    "# ================================\n",
    "df_coord = pd.read_csv(\"conjunto_coordenado_mensagens.csv\")\n",
    "print(\"Pares coordenados:\", len(df_coord))\n",
    "\n",
    "# ================================\n",
    "# 2. Criar grafo\n",
    "# ================================\n",
    "G = nx.Graph()\n",
    "\n",
    "for _, row in df_coord.iterrows():\n",
    "    i = int(row[\"idx_i\"])\n",
    "    j = int(row[\"idx_j\"])\n",
    "    sim = float(row[\"similarity\"])\n",
    "    dt = float(row[\"delta_t_seconds\"])\n",
    "\n",
    "    if G.has_edge(i, j):\n",
    "        G[i][j][\"weight\"] += 1\n",
    "        # opcional: guardar similaridade m√°xima / m√≠nima\n",
    "        G[i][j][\"max_similarity\"] = max(G[i][j][\"max_similarity\"], sim)\n",
    "        G[i][j][\"min_delta_t\"] = min(G[i][j][\"min_delta_t\"], dt)\n",
    "    else:\n",
    "        G.add_edge(\n",
    "            i, j,\n",
    "            weight=1,\n",
    "            similarity=sim,\n",
    "            max_similarity=sim,\n",
    "            delta_t=dt,\n",
    "            min_delta_t=dt\n",
    "        )\n",
    "\n",
    "print(\"N√≥s na rede:\", G.number_of_nodes())\n",
    "print(\"Arestas na rede:\", G.number_of_edges())\n",
    "\n",
    "# ================================\n",
    "# 3. Identificar Componentes Conectados\n",
    "# ================================\n",
    "components = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "print(f\"N√∫mero de componentes coordenados: {len(components)}\")\n",
    "\n",
    "# obter o maior componente coordenado (cluster principal)\n",
    "largest_comp = components[0]\n",
    "print(f\"Tamanho do maior cluster coordenado: {len(largest_comp)} mensagens\")\n",
    "\n",
    "# salvar mensagens desse cluster\n",
    "df_largest = pd.DataFrame({\"message_idx\": list(largest_comp)})\n",
    "df_largest.to_csv(\"cluster_coordenado_principal.csv\", index=False)\n",
    "print(\"Cluster coordenado principal salvo em cluster_coordenado_principal.csv\")\n",
    "\n",
    "# ================================\n",
    "# 4. Salvar Grafo para Gephi / an√°lise externa\n",
    "# ================================\n",
    "nx.write_gexf(G, \"rede_coordenada.gexf\")\n",
    "print(\"Grafo coordenado salvo como rede_coordenada.gexf (compat√≠vel com Gephi).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fe11f8b-5f95-4df6-992f-ed79f1805e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nasci\\AppData\\Local\\Temp\\ipykernel_3204\\3312706711.py:1: DtypeWarning: Columns (11,15,22,23,24,25,26,27) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_msgs = pd.read_csv(\"P1/dataset_zap_1.csv\")  # ou a vers√£o com clean_text\n"
     ]
    }
   ],
   "source": [
    "df_msgs = pd.read_csv(\"dataset_zap_2.csv\")  # ou a vers√£o com clean_text\n",
    "df_msgs = df_msgs.reset_index().rename(columns={\"index\": \"idx\"})\n",
    "\n",
    "df_largest = pd.DataFrame({\"idx\": list(largest_comp)})\n",
    "\n",
    "cluster_msgs = df_largest.merge(df_msgs, on=\"idx\", how=\"left\")\n",
    "\n",
    "cluster_msgs.to_csv(\"cluster_coordenado_principal_com_texto.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b263c53b-e81e-4d1f-8b6c-4d1a79f0a693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pares coordenados no CSV: 78259\n",
      "Pares mapeados para usu√°rios distintos: 70371\n",
      "Falhas de mapeamento (√≠ndices n√£o encontrados): 20\n",
      "Usu√°rios na rede: 5896\n",
      "Arestas (coordena√ß√£o usu√°rio‚Üîusu√°rio): 45770\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# 1. Carregar conjunto coordenado e df preprocessado\n",
    "df_coord = pd.read_csv(\"conjunto_coordenado_mensagens.csv\")\n",
    "df_msgs  = pd.read_csv(\"dataset_zap_2_preprocessado.csv\")\n",
    "\n",
    "# 2. RECRIAR o √≠ndice que foi usado como idx_i / idx_j\n",
    "#    (0, 1, 2, ..., N-1)\n",
    "df_msgs = df_msgs.reset_index().rename(columns={\"index\": \"msg_idx\"})\n",
    "\n",
    "# agora voc√™ tem:\n",
    "# - coluna msg_idx  -> equivalente ao idx_i / idx_j\n",
    "# - coluna id       -> ID original da coleta (se quiser usar depois)\n",
    "# - coluna id_member_anonymous -> usu√°rio\n",
    "\n",
    "# 3. Criar dicion√°rio posi√ß√£o -> usu√°rio\n",
    "df_users = df_msgs[[\"msg_idx\", \"id_member_anonymous\"]].copy()\n",
    "msg_to_user = dict(zip(df_users[\"msg_idx\"], df_users[\"id_member_anonymous\"]))\n",
    "\n",
    "# 4. Construir grafo usu√°rio‚Üîusu√°rio\n",
    "G_users = nx.Graph()\n",
    "\n",
    "pares_ok = 0\n",
    "falhas = 0\n",
    "\n",
    "for _, row in df_coord.iterrows():\n",
    "    i = int(row[\"idx_i\"])\n",
    "    j = int(row[\"idx_j\"])\n",
    "\n",
    "    if i not in msg_to_user or j not in msg_to_user:\n",
    "        falhas += 1\n",
    "        continue\n",
    "\n",
    "    u = msg_to_user[i]\n",
    "    v = msg_to_user[j]\n",
    "\n",
    "    if u == v:\n",
    "        continue\n",
    "\n",
    "    pares_ok += 1\n",
    "\n",
    "    if G_users.has_edge(u, v):\n",
    "        G_users[u][v][\"weight\"] += 1\n",
    "    else:\n",
    "        G_users.add_edge(u, v, weight=1)\n",
    "\n",
    "print(\"Pares coordenados no CSV:\", len(df_coord))\n",
    "print(\"Pares mapeados para usu√°rios distintos:\", pares_ok)\n",
    "print(\"Falhas de mapeamento (√≠ndices n√£o encontrados):\", falhas)\n",
    "print(\"Usu√°rios na rede:\", G_users.number_of_nodes())\n",
    "print(\"Arestas (coordena√ß√£o usu√°rio‚Üîusu√°rio):\", G_users.number_of_edges())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d3383f-f3e1-45a3-a05b-d8ece63fb8fb",
   "metadata": {},
   "source": [
    "### Rede usu√°rio ‚Üí usu√°rio (descobrir quem coordena)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3589182c-524d-4f32-b20b-de411f1e6f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usu√°rios na rede: 5896\n",
      "Arestas (coordena√ß√µes usu√°rio‚Üîusu√°rio): 45770\n",
      "Arestas salvas em: ucn_edges_usuario_usuario.csv\n",
      "N√≥s salvos em: ucn_nodes_usuario_usuario.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# =======================\n",
    "# 1. Carregar dados\n",
    "# =======================\n",
    "df_coord = pd.read_csv(\"conjunto_coordenado_mensagens.csv\")\n",
    "df_msgs  = pd.read_csv(\"dataset_zap_2_preprocessado.csv\")\n",
    "\n",
    "# garantir √≠ndice num√©rico para msgs\n",
    "df_msgs[\"id_msg\"] = df_msgs.index\n",
    "\n",
    "# msg_id ‚Üí user_id\n",
    "df_users = df_msgs[[\"id_msg\", \"id_member_anonymous\"]].copy()\n",
    "msg_to_user = dict(zip(df_users[\"id_msg\"], df_users[\"id_member_anonymous\"]))\n",
    "\n",
    "# =======================\n",
    "# 2. Construir grafo usu√°rio‚Üíusu√°rio\n",
    "# =======================\n",
    "G_users = nx.Graph()\n",
    "\n",
    "for _, row in df_coord.iterrows():\n",
    "    i = int(row[\"idx_i\"])\n",
    "    j = int(row[\"idx_j\"])\n",
    "\n",
    "    if i in msg_to_user and j in msg_to_user:\n",
    "        u = msg_to_user[i]\n",
    "        v = msg_to_user[j]\n",
    "\n",
    "        if u != v:\n",
    "            if G_users.has_edge(u, v):\n",
    "                G_users[u][v][\"weight\"] += 1\n",
    "            else:\n",
    "                G_users.add_edge(u, v, weight=1)\n",
    "\n",
    "\n",
    "print(\"Usu√°rios na rede:\", G_users.number_of_nodes())\n",
    "print(\"Arestas (coordena√ß√µes usu√°rio‚Üîusu√°rio):\", G_users.number_of_edges())\n",
    "\n",
    "# =======================\n",
    "# 3. Criar DataFrame de ARESTAS\n",
    "# =======================\n",
    "edges_data = []\n",
    "\n",
    "for u, v, data in G_users.edges(data=True):\n",
    "    edges_data.append({\n",
    "        \"user_u\": u,\n",
    "        \"user_v\": v,\n",
    "        \"weight\": data[\"weight\"]\n",
    "    })\n",
    "\n",
    "df_edges = pd.DataFrame(edges_data)\n",
    "df_edges.to_csv(\"ucn_edges_usuario_usuario.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Arestas salvas em: ucn_edges_usuario_usuario.csv\")\n",
    "\n",
    "# =======================\n",
    "# 4. Criar DataFrame de N√ìS\n",
    "# =======================\n",
    "nodes_data = []\n",
    "\n",
    "# grau simples\n",
    "degree = dict(G_users.degree())\n",
    "\n",
    "# grau ponderado\n",
    "degree_weighted = dict(G_users.degree(weight=\"weight\"))\n",
    "\n",
    "for n in G_users.nodes():\n",
    "    nodes_data.append({\n",
    "        \"user_id\": n,\n",
    "        \"degree\": degree.get(n, 0),\n",
    "        \"degree_weighted\": degree_weighted.get(n, 0)\n",
    "    })\n",
    "\n",
    "df_nodes = pd.DataFrame(nodes_data)\n",
    "df_nodes.to_csv(\"ucn_nodes_usuario_usuario.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"N√≥s salvos em: ucn_nodes_usuario_usuario.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a8e188-7895-4f2b-b548-d02b9bfc8154",
   "metadata": {},
   "source": [
    "### Rede grupo ‚Üí grupo (quais grupos participam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a8631f9-e511-42ec-87a0-e18bf284f417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grupos na rede: 269\n",
      "Arestas (coordena√ß√µes grupo‚Üîgrupo): 8437\n",
      "Arestas grupo‚Üígrupo salvas em: gcn_edges_grupo_grupo.csv\n",
      "N√≥s (grupos) salvos em: gcn_nodes_grupo_grupo.csv\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# =======================\n",
    "# 1. Carregar dados\n",
    "# =======================\n",
    "df_coord = pd.read_csv(\"conjunto_coordenado_mensagens.csv\")\n",
    "df_msgs  = pd.read_csv(\"dataset_zap_2_preprocessado.csv\")\n",
    "\n",
    "# garantir √≠ndice num√©rico para msgs\n",
    "df_msgs[\"id_message\"] = df_msgs.index\n",
    "\n",
    "# manter apenas colunas relevantes\n",
    "df_groups = df_msgs[[\"id_message\", \"id_group_anonymous\"]].copy()\n",
    "\n",
    "# opcional: remover linhas sem grupo (caso existam)\n",
    "df_groups = df_groups.dropna(subset=[\"id_group_anonymous\"])\n",
    "\n",
    "# dicion√°rio msg_id ‚Üí group_id\n",
    "msg_to_group = dict(zip(df_groups[\"id_message\"], df_groups[\"id_group_anonymous\"]))\n",
    "\n",
    "# =======================\n",
    "# 2. Construir grafo grupo‚Üígrupo\n",
    "# =======================\n",
    "G_groups = nx.Graph()\n",
    "\n",
    "for _, row in df_coord.iterrows():\n",
    "    i = int(row[\"idx_i\"])\n",
    "    j = int(row[\"idx_j\"])\n",
    "\n",
    "    # ignorar se n√£o temos grupo para alguma das mensagens\n",
    "    if i not in msg_to_group or j not in msg_to_group:\n",
    "        continue\n",
    "\n",
    "    g1 = msg_to_group[i]\n",
    "    g2 = msg_to_group[j]\n",
    "\n",
    "    # ignorar coordena√ß√£o dentro do mesmo grupo (se quiser s√≥ inter-grupos)\n",
    "    if g1 == g2:\n",
    "        continue\n",
    "\n",
    "    # peso = n√∫mero de mensagens coordenadas entre os dois grupos\n",
    "    if G_groups.has_edge(g1, g2):\n",
    "        G_groups[g1][g2][\"weight\"] += 1\n",
    "    else:\n",
    "        G_groups.add_edge(g1, g2, weight=1)\n",
    "\n",
    "print(\"Grupos na rede:\", G_groups.number_of_nodes())\n",
    "print(\"Arestas (coordena√ß√µes grupo‚Üîgrupo):\", G_groups.number_of_edges())\n",
    "\n",
    "# =======================\n",
    "# 3. DataFrame de ARESTAS (grupo‚Üígrupo)\n",
    "# =======================\n",
    "edges_data = []\n",
    "\n",
    "for g1, g2, data in G_groups.edges(data=True):\n",
    "    edges_data.append({\n",
    "        \"group_a\": g1,\n",
    "        \"group_b\": g2,\n",
    "        \"weight\": data[\"weight\"]\n",
    "    })\n",
    "\n",
    "df_edges_groups = pd.DataFrame(edges_data)\n",
    "df_edges_groups.to_csv(\"gcn_edges_grupo_grupo.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Arestas grupo‚Üígrupo salvas em: gcn_edges_grupo_grupo.csv\")\n",
    "\n",
    "# =======================\n",
    "# 4. DataFrame de N√ìS (grupos)\n",
    "# =======================\n",
    "nodes_data = []\n",
    "\n",
    "# grau simples e ponderado\n",
    "degree = dict(G_groups.degree())\n",
    "degree_weighted = dict(G_groups.degree(weight=\"weight\"))\n",
    "\n",
    "for g in G_groups.nodes():\n",
    "    nodes_data.append({\n",
    "        \"group_id\": g,\n",
    "        \"degree\": degree.get(g, 0),\n",
    "        \"degree_weighted\": degree_weighted.get(g, 0)\n",
    "    })\n",
    "\n",
    "df_nodes_groups = pd.DataFrame(nodes_data)\n",
    "df_nodes_groups.to_csv(\"gcn_nodes_grupo_grupo.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"N√≥s (grupos) salvos em: gcn_nodes_grupo_grupo.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84c4704-16b1-4a9b-b98c-b2169d2c9309",
   "metadata": {},
   "source": [
    "### APLICAR ALGORITMOS DE AGRUPAMENTO / COMUNIDADES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f5c4de9-bac4-4166-90a9-04af748bd7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-louvain\n",
      "  Downloading python-louvain-0.16.tar.gz (204 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: networkx in c:\\users\\nasci\\documentos\\dev\\env\\proj\\.venv\\lib\\site-packages (from python-louvain) (3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\nasci\\documentos\\dev\\env\\proj\\.venv\\lib\\site-packages (from python-louvain) (1.26.4)\n",
      "Building wheels for collected packages: python-louvain\n",
      "  Building wheel for python-louvain (setup.py): started\n",
      "  Building wheel for python-louvain (setup.py): finished with status 'done'\n",
      "  Created wheel for python-louvain: filename=python_louvain-0.16-py3-none-any.whl size=9473 sha256=be08eea1535e82afa9562b95fc22fb6ae3d7453f2687eb7eff8f49447dc959fc\n",
      "  Stored in directory: c:\\users\\nasci\\appdata\\local\\pip\\cache\\wheels\\40\\f1\\e3\\485b698c520fa0baee1d07897abc7b8d6479b7d199ce96f4af\n",
      "Successfully built python-louvain\n",
      "Installing collected packages: python-louvain\n",
      "Successfully installed python-louvain-0.16\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: Building 'python-louvain' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'python-louvain'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n",
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\nasci\\Documentos\\Dev\\Env\\proj\\.venv\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install python-louvain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b43c7658-75fa-413a-af57-4d76868be207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from community import community_louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ad6a885-1fdf-459f-b387-b047addeb1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Rodando Louvain em ucn ===\n",
      "N√≥s: 5896\n",
      "Arestas: 45770\n",
      "Executando algoritmo de Louvain...\n",
      "Comunidades salvas em: ucn_communities.csv\n",
      "Tamanhos das comunidades salvos em: ucn_community_sizes.csv\n",
      "\n",
      "Top comunidades por tamanho:\n",
      "   community  size\n",
      "0         21   705\n",
      "1         16   577\n",
      "2          2   564\n",
      "3         26   506\n",
      "4          5   403\n",
      "\n",
      "=== Rodando Louvain em gcn ===\n",
      "N√≥s: 269\n",
      "Arestas: 8437\n",
      "Executando algoritmo de Louvain...\n",
      "Comunidades salvas em: gcn_communities.csv\n",
      "Tamanhos das comunidades salvos em: gcn_community_sizes.csv\n",
      "\n",
      "Top comunidades por tamanho:\n",
      "   community  size\n",
      "0          1   111\n",
      "1          0    85\n",
      "2          6    44\n",
      "3          5    22\n",
      "4          3     3\n"
     ]
    }
   ],
   "source": [
    "import community  # python-louvain\n",
    "\n",
    "# ============================\n",
    "# Fun√ß√£o para rodar Louvain\n",
    "# ============================\n",
    "def run_louvain(nodes_csv, edges_csv, node_id_col, output_prefix):\n",
    "    print(f\"\\n=== Rodando Louvain em {output_prefix} ===\")\n",
    "\n",
    "    # Carregar dados\n",
    "    df_nodes = pd.read_csv(nodes_csv)\n",
    "    df_edges = pd.read_csv(edges_csv)\n",
    "\n",
    "    # Construir grafo\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Adicionar n√≥s\n",
    "    for _, row in df_nodes.iterrows():\n",
    "        G.add_node(row[node_id_col])\n",
    "\n",
    "    # Adicionar arestas com peso\n",
    "    for _, row in df_edges.iterrows():\n",
    "        G.add_edge(row[df_edges.columns[0]],\n",
    "                   row[df_edges.columns[1]],\n",
    "                   weight=row[\"weight\"])\n",
    "\n",
    "    print(\"N√≥s:\", G.number_of_nodes())\n",
    "    print(\"Arestas:\", G.number_of_edges())\n",
    "\n",
    "    # ============================\n",
    "    # Executar Louvain\n",
    "    # ============================\n",
    "    print(\"Executando algoritmo de Louvain...\")\n",
    "    partition = community.best_partition(G, weight=\"weight\")\n",
    "\n",
    "    # Criar DF com resultado da comunidade\n",
    "    df_comm = pd.DataFrame({\n",
    "        node_id_col: list(partition.keys()),\n",
    "        \"community\": list(partition.values())\n",
    "    })\n",
    "\n",
    "    # Salvar CSV com comunidades\n",
    "    df_comm.to_csv(f\"{output_prefix}_communities.csv\", index=False, encoding=\"utf-8\")\n",
    "    print(f\"Comunidades salvas em: {output_prefix}_communities.csv\")\n",
    "\n",
    "    # Estat√≠sticas de tamanho por comunidade\n",
    "    df_stats = df_comm[\"community\"].value_counts().reset_index()\n",
    "    df_stats.columns = [\"community\", \"size\"]\n",
    "\n",
    "    df_stats.to_csv(f\"{output_prefix}_community_sizes.csv\", index=False, encoding=\"utf-8\")\n",
    "    print(f\"Tamanhos das comunidades salvos em: {output_prefix}_community_sizes.csv\")\n",
    "\n",
    "    print(\"\\nTop comunidades por tamanho:\")\n",
    "    print(df_stats.head())\n",
    "\n",
    "    return df_comm, df_stats\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 1. Louvain na rede usu√°rio‚Üíusu√°rio (UCN)\n",
    "# ============================\n",
    "df_ucn_comm, df_ucn_stats = run_louvain(\n",
    "    nodes_csv=\"ucn_nodes_usuario_usuario.csv\",\n",
    "    edges_csv=\"ucn_edges_usuario_usuario.csv\",\n",
    "    node_id_col=\"user_id\",\n",
    "    output_prefix=\"ucn\"\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# 2. Louvain na rede grupo‚Üígrupo (GCN)\n",
    "# ============================\n",
    "df_gcn_comm, df_gcn_stats = run_louvain(\n",
    "    nodes_csv=\"gcn_nodes_grupo_grupo.csv\",\n",
    "    edges_csv=\"gcn_edges_grupo_grupo.csv\",\n",
    "    node_id_col=\"group_id\",\n",
    "    output_prefix=\"gcn\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c24402d-913d-452a-962f-de257804ae9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UCN - n√≥s: 5896 arestas: 45770\n",
      "GCN - n√≥s: 269 arestas: 8437\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import community  # python-louvain\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 6)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "\n",
    "# -------------------------\n",
    "# UCN: usu√°rio ‚Üí usu√°rio\n",
    "# -------------------------\n",
    "ucn_nodes = pd.read_csv(\"ucn_nodes_usuario_usuario.csv\")        # user_id, degree, degree_weighted\n",
    "ucn_edges = pd.read_csv(\"ucn_edges_usuario_usuario.csv\")        # user_u, user_v, weight\n",
    "ucn_comms = pd.read_csv(\"ucn_communities.csv\")                  # user_id, community\n",
    "\n",
    "# mesclar comunidades nos n√≥s\n",
    "ucn_nodes = ucn_nodes.merge(ucn_comms, on=\"user_id\", how=\"left\")\n",
    "\n",
    "# construir grafo UCN\n",
    "G_ucn = nx.Graph()\n",
    "for _, r in ucn_nodes.iterrows():\n",
    "    G_ucn.add_node(r[\"user_id\"])\n",
    "\n",
    "for _, r in ucn_edges.iterrows():\n",
    "    G_ucn.add_edge(r[\"user_u\"], r[\"user_v\"], weight=r[\"weight\"])\n",
    "\n",
    "print(\"UCN - n√≥s:\", G_ucn.number_of_nodes(), \"arestas:\", G_ucn.number_of_edges())\n",
    "\n",
    "# -------------------------\n",
    "# GCN: grupo ‚Üí grupo\n",
    "# -------------------------\n",
    "gcn_nodes = pd.read_csv(\"gcn_nodes_grupo_grupo.csv\")            # group_id, degree, degree_weighted\n",
    "gcn_edges = pd.read_csv(\"gcn_edges_grupo_grupo.csv\")            # group_u, group_v, weight\n",
    "gcn_comms = pd.read_csv(\"gcn_communities.csv\")                  # group_id, community\n",
    "\n",
    "gcn_nodes = gcn_nodes.merge(gcn_comms, on=\"group_id\", how=\"left\")\n",
    "\n",
    "G_gcn = nx.Graph()\n",
    "for _, r in gcn_nodes.iterrows():\n",
    "    G_gcn.add_node(r[\"group_id\"])\n",
    "\n",
    "for _, r in gcn_edges.iterrows():\n",
    "    G_gcn.add_edge(r[gcn_edges.columns[0]], r[gcn_edges.columns[1]], weight=r[\"weight\"])\n",
    "\n",
    "print(\"GCN - n√≥s:\", G_gcn.number_of_nodes(), \"arestas:\", G_gcn.number_of_edges())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f0ebe2e-5254-494b-82f7-82d15c47d220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>degree</th>\n",
       "      <th>degree_weighted</th>\n",
       "      <th>community</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>b6242381059f781a69bc6d7b4803c25b</td>\n",
       "      <td>435</td>\n",
       "      <td>2232</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>cd45e9e743ddcb9a94f61619300539a9</td>\n",
       "      <td>703</td>\n",
       "      <td>2221</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>c5fff63b6151f93e1ce86d12f8acbee1</td>\n",
       "      <td>629</td>\n",
       "      <td>1965</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>cebd2107ff2001db85851cc0e81e0667</td>\n",
       "      <td>378</td>\n",
       "      <td>1820</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1f062a784173f6f9b4692c192a091e9d</td>\n",
       "      <td>523</td>\n",
       "      <td>1814</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>ef496106907ece8b169c6219e5470b2c</td>\n",
       "      <td>513</td>\n",
       "      <td>1037</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>f6dc94bc9de9e8084e40cbfbdd505384</td>\n",
       "      <td>431</td>\n",
       "      <td>947</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>2f977bd09af5c0f9b1f4461992011304</td>\n",
       "      <td>292</td>\n",
       "      <td>924</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4ea1b3ee637da811b7d2d0df32db21f9</td>\n",
       "      <td>448</td>\n",
       "      <td>875</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>6612d5e54818010eb9de990dcbe87384</td>\n",
       "      <td>484</td>\n",
       "      <td>871</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              user_id  degree  degree_weighted  community\n",
       "17   b6242381059f781a69bc6d7b4803c25b     435             2232          5\n",
       "82   cd45e9e743ddcb9a94f61619300539a9     703             2221          2\n",
       "92   c5fff63b6151f93e1ce86d12f8acbee1     629             1965          2\n",
       "25   cebd2107ff2001db85851cc0e81e0667     378             1820         26\n",
       "22   1f062a784173f6f9b4692c192a091e9d     523             1814         26\n",
       "68   ef496106907ece8b169c6219e5470b2c     513             1037          2\n",
       "120  f6dc94bc9de9e8084e40cbfbdd505384     431              947          9\n",
       "547  2f977bd09af5c0f9b1f4461992011304     292              924         92\n",
       "14   4ea1b3ee637da811b7d2d0df32db21f9     448              875          2\n",
       "178  6612d5e54818010eb9de990dcbe87384     484              871        119"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top50_deg = (\n",
    "    ucn_nodes.sort_values(\"degree_weighted\", ascending=False)\n",
    "             .head(50)\n",
    ")\n",
    "\n",
    "top50_deg.to_csv(\"ucn_top50_degree_weighted.csv\", index=False, encoding=\"utf-8\")\n",
    "display(top50_deg.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87761008-0053-4c26-9159-a82edfcf2638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculando betweenness centrality (pode demorar um pouco)...\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculando betweenness centrality (pode demorar um pouco)...\")\n",
    "bet = nx.betweenness_centrality(G_ucn, weight=\"weight\", normalized=True)\n",
    "\n",
    "bet_df = pd.DataFrame({\n",
    "    \"user_id\": list(bet.keys()),\n",
    "    \"betweenness\": list(bet.values())\n",
    "})\n",
    "\n",
    "bet_df = bet_df.merge(\n",
    "    ucn_nodes[[\"user_id\", \"degree_weighted\", \"community\"]],\n",
    "    on=\"user_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "top50_bet = bet_df.sort_values(\"betweenness\", ascending=False).head(50)\n",
    "top50_bet.to_csv(\"ucn_top50_betweenness.csv\", index=False, encoding=\"utf-8\")\n",
    "display(top50_bet.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfce97d6-678c-415a-94db-079c6ec42870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maiores comunidades globais\n",
    "comm_sizes_ucn = ucn_nodes[\"community\"].value_counts()\n",
    "print(\"Tamanhos das maiores comunidades UCN:\")\n",
    "display(comm_sizes_ucn.head(10))\n",
    "\n",
    "top_parent_comms = comm_sizes_ucn.head(3).index.tolist()\n",
    "\n",
    "sub_results = []\n",
    "\n",
    "for c in top_parent_comms:\n",
    "    print(f\"\\n>> Rodando Louvain dentro da comunidade global {c}\")\n",
    "    nodes_c = ucn_nodes[ucn_nodes[\"community\"] == c][\"user_id\"].tolist()\n",
    "    subG = G_ucn.subgraph(nodes_c).copy()\n",
    "    \n",
    "    if subG.number_of_nodes() < 5:\n",
    "        print(\"Comunidade muito pequena, pulando.\")\n",
    "        continue\n",
    "    \n",
    "    part_sub = community.best_partition(subG, weight=\"weight\")\n",
    "    \n",
    "    sub_df = pd.DataFrame({\n",
    "        \"user_id\": list(part_sub.keys()),\n",
    "        \"community_parent\": c,\n",
    "        \"community_sub\": [f\"{c}_{p}\" for p in part_sub.values()]\n",
    "    })\n",
    "    sub_results.append(sub_df)\n",
    "\n",
    "if sub_results:\n",
    "    df_sub_comms = pd.concat(sub_results, ignore_index=True)\n",
    "    df_sub_comms.to_csv(\"ucn_subcommunities_recursive.csv\", index=False, encoding=\"utf-8\")\n",
    "    print(\"\\nSubcomunidades internas salvas em 'ucn_subcommunities_recursive.csv'\")\n",
    "    display(df_sub_comms.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccc03d0-9087-441e-b832-f87a6d8faeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecionar subconjunto de n√≥s (ex: top 300 por degree_weighted)\n",
    "subset_nodes = (\n",
    "    ucn_nodes.sort_values(\"degree_weighted\", ascending=False)\n",
    "             .head(300)[\"user_id\"]\n",
    "             .tolist()\n",
    ")\n",
    "\n",
    "H = G_ucn.subgraph(subset_nodes).copy()\n",
    "\n",
    "# layout Kamada-Kawai\n",
    "pos = nx.kamada_kawai_layout(H)\n",
    "\n",
    "# mapear atributos\n",
    "comm_map = ucn_nodes.set_index(\"user_id\")[\"community\"].to_dict()\n",
    "deg_map  = ucn_nodes.set_index(\"user_id\")[\"degree_weighted\"].to_dict()\n",
    "\n",
    "communities = [comm_map[n] for n in H.nodes()]\n",
    "degrees_w  = np.array([deg_map[n] for n in H.nodes()])\n",
    "\n",
    "# tamanhos de n√≥ (escala)\n",
    "node_sizes = 50 + 5 * np.sqrt(degrees_w)\n",
    "\n",
    "# cores por comunidade\n",
    "unique_comms = sorted(set(communities))\n",
    "color_map = {c: cm.tab20(i % 20) for i, c in enumerate(unique_comms)}\n",
    "node_colors = [color_map[c] for c in communities]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "nx.draw_networkx_edges(H, pos, alpha=0.2)\n",
    "nx.draw_networkx_nodes(\n",
    "    H, pos,\n",
    "    node_size=node_sizes,\n",
    "    node_color=node_colors,\n",
    "    alpha=0.9\n",
    ")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"UCN ‚Äî top 300 usu√°rios por grau ponderado (cores = comunidades)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ucn_bubbles_top300.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f33a3c3-42fe-44e3-a1e2-83ab5150fed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_groups_deg = (\n",
    "    gcn_nodes.sort_values(\"degree_weighted\", ascending=False)\n",
    "             .head(50)\n",
    ")\n",
    "\n",
    "top_groups_deg.to_csv(\"gcn_top50_degree_weighted.csv\", index=False, encoding=\"utf-8\")\n",
    "display(top_groups_deg.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d21359-9e54-4554-bbb1-7932ae63c12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset preprocessado com datas\n",
    "df_msgs = pd.read_csv(\"dataset_zap_2_preprocessado.csv\", parse_dates=[\"date_message\"])\n",
    "\n",
    "# manter apenas colunas relevantes\n",
    "df_groups_time = df_msgs[[\"id_group_anonymous\", \"date_message\"]].dropna().copy()\n",
    "\n",
    "# juntar com comunidades de grupos\n",
    "df_groups_time = df_groups_time.merge(\n",
    "    gcn_comms,\n",
    "    left_on=\"id_group_anonymous\",\n",
    "    right_on=\"group_id\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# extrair apenas a data (dia)\n",
    "df_groups_time[\"date\"] = df_groups_time[\"date_message\"].dt.date\n",
    "\n",
    "# s√©rie temporal: n¬∫ de mensagens por comunidade por dia\n",
    "timeline = (\n",
    "    df_groups_time.groupby([\"community\", \"date\"])\n",
    "                  .size()\n",
    "                  .reset_index(name=\"num_messages\")\n",
    ")\n",
    "\n",
    "timeline.to_csv(\"gcn_timeline_community_day.csv\", index=False, encoding=\"utf-8\")\n",
    "display(timeline.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0604969c-68ee-4a9f-bb11-34f3379c95b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pegar as 3 maiores comunidades de grupos\n",
    "comm_sizes_gcn = gcn_nodes[\"community\"].value_counts()\n",
    "top_comm_gcn = comm_sizes_gcn.head(3).index.tolist()\n",
    "print(\"Maiores comunidades GCN:\", top_comm_gcn)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for c in top_comm_gcn:\n",
    "    sub = timeline[timeline[\"community\"] == c].sort_values(\"date\")\n",
    "    plt.plot(sub[\"date\"], sub[\"num_messages\"], marker=\"o\", label=f\"Community {c}\")\n",
    "\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Number of messages\")\n",
    "plt.title(\"GCN ‚Äî message volume per community (daily)\")\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"gcn_community_timelines.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02199f42-ee7b-448e-8c84-f5a7a179feb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ucn_sizes = pd.read_csv(\"ucn_community_sizes.csv\")  # community, size\n",
    "gcn_sizes = pd.read_csv(\"gcn_community_sizes.csv\")  # community, size\n",
    "\n",
    "# UCN\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(ucn_sizes[\"size\"], bins=30)\n",
    "plt.xlabel(\"Community size (users)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"UCN ‚Äî distribution of community sizes\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ucn_community_size_distribution.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# GCN\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(gcn_sizes[\"size\"], bins=20)\n",
    "plt.xlabel(\"Community size (groups)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"GCN ‚Äî distribution of community sizes\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"gcn_community_size_distribution.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a3ebbf-b535-4f12-b41f-0cdb3368ef09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pegar todos os grupos ou apenas um subconjunto (ex: top 200 por degree)\n",
    "subset_groups = (\n",
    "    gcn_nodes.sort_values(\"degree_weighted\", ascending=False)\n",
    "             .head(200)[\"group_id\"]\n",
    "             .tolist()\n",
    ")\n",
    "\n",
    "H_g = G_gcn.subgraph(subset_groups).copy()\n",
    "pos_g = nx.kamada_kawai_layout(H_g)\n",
    "\n",
    "comm_map_g = gcn_nodes.set_index(\"group_id\")[\"community\"].to_dict()\n",
    "deg_map_g  = gcn_nodes.set_index(\"group_id\")[\"degree_weighted\"].to_dict()\n",
    "\n",
    "communities_g = [comm_map_g[n] for n in H_g.nodes()]\n",
    "degw_g       = np.array([deg_map_g[n] for n in H_g.nodes()])\n",
    "sizes_g      = 60 + 6 * np.sqrt(degw_g)\n",
    "\n",
    "unique_comms_g = sorted(set(communities_g))\n",
    "color_map_g = {c: cm.tab20(i % 20) for i, c in enumerate(unique_comms_g)}\n",
    "colors_g = [color_map_g[c] for c in communities_g]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "nx.draw_networkx_edges(H_g, pos_g, alpha=0.25)\n",
    "nx.draw_networkx_nodes(H_g, pos_g, node_size=sizes_g, node_color=colors_g, alpha=0.95)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"GCN ‚Äî top 200 groups (colors = Louvain communities)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"gcn_bubbles_top200.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69722af6-2ea5-450f-ad32-50dc918b1533",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ucn_sizes.sort_values(\"size\", ascending=False).head(10).to_latex(\n",
    "    index=False,\n",
    "    caption=\"Largest communities in the user coordination network (UCN) detected by Louvain.\",\n",
    "    label=\"tab:ucn_communities\"\n",
    "))\n",
    "\n",
    "print(gcn_sizes.sort_values(\"size\", ascending=False).head(10).to_latex(\n",
    "    index=False,\n",
    "    caption=\"Largest communities in the group coordination network (GCN) detected by Louvain.\",\n",
    "    label=\"tab:gcn_communities\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a19d14e-0fa0-447d-b1f2-b121c99d1d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"dataset_zap_2_preprocessado.csv\", low_memory=False)\n",
    "\n",
    "# garantir que a coluna √© datetime\n",
    "df[\"date_message\"] = pd.to_datetime(df[\"date_message\"], errors=\"coerce\")\n",
    "\n",
    "# remover datas inv√°lidas\n",
    "df = df.dropna(subset=[\"date_message\"]).copy()\n",
    "\n",
    "# criar coluna apenas com a data (dia)\n",
    "df[\"day\"] = df[\"date_message\"].dt.date\n",
    "\n",
    "print(\"Intervalo temporal:\", df[\"date_message\"].min(), \"‚Üí\", df[\"date_message\"].max())\n",
    "print(\"Total de mensagens:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a27f271-1b2d-4bb5-ad44-b011b265aed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_per_day = df.groupby(\"day\").size().reset_index(name=\"num_messages\")\n",
    "\n",
    "print(msg_per_day.head())\n",
    "print(\"\\nDias com mais mensagens:\")\n",
    "display(msg_per_day.sort_values(\"num_messages\", ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3f3023-03c1-459d-a022-45a695c4e3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(msg_per_day[\"day\"], msg_per_day[\"num_messages\"], marker=\"o\", linewidth=1)\n",
    "plt.title(\"Volume de mensagens por dia\")\n",
    "plt.xlabel(\"Dia\")\n",
    "plt.ylabel(\"N√∫mero de mensagens\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd2dded-a2e0-4632-a2aa-b4502f4c648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eleicao = pd.to_datetime(\"2022-10-30\").date()\n",
    "\n",
    "mensagens_eleicao = df[df[\"day\"] == eleicao]\n",
    "\n",
    "print(\"Total de mensagens no dia da elei√ß√£o:\", len(mensagens_eleicao))\n",
    "mensagens_eleicao.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006a1ab2-2a2c-4234-b6cd-430a3de52206",
   "metadata": {},
   "outputs": [],
   "source": [
    "mensagens_eleicao[[\"date_message\", \"text_content_anonymous\"]].sample(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06b9ebc-8aa3-4189-a0b5-376e917a6cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "periodo = df[\n",
    "    (df[\"day\"] >= pd.to_datetime(\"2022-10-20\").date()) &\n",
    "    (df[\"day\"] <= pd.to_datetime(\"2022-11-10\").date())\n",
    "]\n",
    "\n",
    "periodo_count = periodo.groupby(\"day\").size().reset_index(name=\"num_messages\")\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.bar(periodo_count[\"day\"], periodo_count[\"num_messages\"])\n",
    "plt.axvline(pd.to_datetime(\"2022-10-30\"), color=\"red\", linestyle=\"--\", label=\"Elei√ß√£o 30/10/22\")\n",
    "plt.title(\"Volume de mensagens (20/10 ‚Äì 10/11)\")\n",
    "plt.xlabel(\"Dia\")\n",
    "plt.ylabel(\"N¬∫ de mensagens\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fa3396-12cb-4133-b6f9-858eeefebbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "top3 = msg_per_day.sort_values(\"num_messages\", ascending=False).head(3)\n",
    "print(\"Top 3 dias com mais mensagens:\")\n",
    "display(top3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed88b47e-2ef1-4fe4-b17b-60842d08544d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (.venv)",
   "language": "python",
   "name": "py312-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
